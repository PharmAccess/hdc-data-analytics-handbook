# Datawrangling
In data analysis, we often repeat similar actions to get valuable information from the data. In this Chapter we describe the most common techniques applied in data analysis, and how they are performed in different programming languages and libraries.

## Split-apply-combine
Many data analysis problems involve the application of a split-apply-combine strategy,
where you break up a big problem into manageable pieces, operate on each piece independently and then put all the pieces back together. group_by is the  most common function name for split-apply-combine technique.

These actions can be done with any library, in multiple programming languages. 
Based on the sample data created in [Synthea patient generator](synthea.qmd) and [Create views using DuckDB](duckdb.qmd), we will go through some standard data analysis actions. 
We will show how to apply these actions in multiple libraries and languages. 

## Datawrangling languages and libraries
In this chapter we use the following languages and libraries:

**Pandas 2.0**  
[Pandas]() is a popular python data wrangling library in the data science comunity. Pandas 2.0 mainly introduces improvements in performance, making it faster and more memory-efficient. 

**Polars**  
Polars is very similar to pandas in terms of functionality. However, polars is based on Rust, which makes it highly performant.

**IBIS**  
Ibis is a python library which enables you to write your code in a pandas-like way, and the translates it to a lot of different backends. This enables you to change backends with only minor adjustments to your data wrangling code.

**PySpark**
[PySpark]() is a python library for Apache Spark, an open source computing system. It enables you to work with large datasets, or run complex calculations. 

**dplyr**  
dplyr is a widely used library in r.

**SQL - DuckDB**  
In this handbook we use DuckDB to show how to perform datawrangling with SQL. DuckDB SQL is almost the same as plane SQL but has a couple of extra functions to handle nested data and a couple of functions that help with data analysis. [DuckDB]() is **TODO**

## Overview data transformations in different libraries
In the table below, you can find an overview of techniques that are often used in data analysis and the corresponding functions for them in the discussed libraries/languages.

| concept | pandas | Polars | IBIS | PySpark | dplyr | SQL |
| --- | --- | ---| --- | --- | --- | --- |
| *Split* | groupby() | group_by() | group_by() | groupBy() | group_by() | GROUP BY |
| *Apply* | many functions | many functions | many functions | many functions | many functions | many functions | 
| *combine* | join (), merge() | join() | left_join, inner_join() etc. |  join() | left_join, inner_join() etc. | LEFT JOIN, JOIN etc. |
| *Filtering (row based)*| loc[], query() | filter() | filter() | filter() | filter() | WHERE | 
| *Filtering (column based)*| loc[], iloc[],| select() | - | select() | select() | SELECT | 
| *Mutate* | assign() | with_columns() | mutuate() | withColumn() | mutate() | ADD | 
| *Ordering* | sort_values() | sort() | order_by() | orderBy() | arrange() | ORDER BY | 



## Example using the Synthea dataset
To demonstrate the most common data analysis techniques in the different languages and libraries, we use the data generated with the [Synthea Patient Generator](synthea.qmd).

### load required libraries
Before getting started, we need to load the required libraries. 

::: {.panel-tabset}

## pandas

```{python}
import pandas as pd
```


## polars
```{python}
import polars as pl
```


## SQL

```{python}
import duckdb
```

## IBIS
```{python}
import ibis 
import pandas as pd
import duckdb
```

## pyspark

```{python}
import pyspark
from pyspark.sql import SparkSession
from pyspark.sql.types import StructType,StructField, StringType, DateType,TimestampType,IntegerType
from pyspark.sql import functions as F
```

## dplyr

```{python}
%load_ext rpy2.ipython
```

```{python} 
%%R
library(DBI)
library(dplyr)
library(duckdb)
library(glue)
library(lubridate)
library(knitr)
```

:::

### Retrieve data

In this section we will retrieve the data from the parquet files created in [Creating views with DuckDB](duckdb.qmd).

Using parquet files enables us to separate storage and compute. This provides us with the flexibility to (for example) scale out storage engine, or change our compute backend, without affecting the other.

```{dot}
graph storage_compute {
    node [shape = box];
    storage -- compute;
}
```

First, we set the paths where the data is located

:::{.panel-tabset}

## Python

```{python}
# set paths
#from pathlib import Path

# TODO: is there a better way?
# path from which code is run
# when running entire notebook, this is from the main folder, 
# when running file locally, this is from the source folder
#ROOT = Path('.')
#if 'src' in str(ROOT.resolve()): # locally
#    ROOT = '..' / ROOT # go one folder up
#BRONZE = ROOT / 'data' / 'bronze'
#SILVER = ROOT / 'data' / 'silver'
```

```{python}
BRONZE = 'data/bronze'
SILVER = 'data/silver'
```

## R

```{python}
%%R
library(here)

ROOT <- here()


# Check if the 'src' directory is in the path and adjust the ROOT if needed
if (grepl("src", ROOT, fixed = TRUE)) {
  ROOT <- dirname(ROOT)
}

# Define BRONZE and SILVER directories
BRONZE <- file.path(ROOT, "data", "bronze")
SILVER <- file.path(ROOT, "data", "silver")
```

:::
Thereafter, we import the data in a dataframe

::: {.panel-tabset}
## pandas

```{python}
# load data from parquet
df_timeline_pandas = pd.read_parquet(f"{SILVER}/parquet_export/patient_timeline.parquet")
df_price_list_pandas = pd.read_parquet(f"{SILVER}/parquet_export/price_list.parquet")

```

## polars
```{python}
# read parquet files in polars dateframe
df_timeline_polars = pl.read_parquet(f"{SILVER}/parquet_export/patient_timeline.parquet")
df_price_list_polars = pl.read_parquet(f"{SILVER}/parquet_export/price_list.parquet")
```

## SQL

In [Creating views using DuckDB](duckdb.qmd), we already showed how to create a DuckDB database and read in parquet data in duckdb. Therefore we will connect here to the already created database with soley the patient_timeline and price_list table available.
```{python}
# connect to the database
con_duckdb = duckdb.connect()

con_duckdb.sql(
    f"""
    CREATE TABLE patient_timeline AS
    SELECT *
    FROM read_parquet('{SILVER}/parquet_export/patient_timeline.parquet');
    """
)

con_duckdb.sql(
    f"""
    CREATE TABLE price_list AS
    SELECT *
    FROM read_parquet('{SILVER}/parquet_export/price_list.parquet');
    """
)
```


## IBIS

**TODO** Show that this is also possible directly on parquet files
```{python}
# read parquet files with ibis
df_timeline_IBIS = ibis.read_parquet(f'{SILVER}/parquet_export/patient_timeline.parquet')
df_price_list_ibis = ibis.read_parquet(f'{SILVER}/parquet_export/price_list.parquet')
```

## pyspark

```{python}
# create spark session
spark = SparkSession.builder.appName("patient_timeline_analysis").getOrCreate()

# convert timeline to spark dataframe
df_timeline_spark = spark.read.parquet(f'{SILVER}/parquet_export/patient_timeline.parquet')

# convert pricelist to spark dataframe
df_price_list_spark = spark.read.parquet(f'{SILVER}/parquet_export/price_list.parquet')

```

## dplyr
```{python}
%%R
con <- dbConnect(
  duckdb::duckdb(),
  dbdir = glue("{SILVER}/pregnancyR.duckdb"),
  read_only = TRUE
)
```


```{python}
%%R
df <- dbGetQuery(con, "SELECT * FROM patient_timeline")
df_price_list <- dbGetQuery(con, "SELECT * FROM price_list")
```

:::

### Split-apply-combine
From the patient timeline data, we want to know how many patients each organization has recorded. For this task we can use the split-apply-combine technique. 

* we **split** the data in groups, where each group is a organization
* we **apply** a function to count the number of patients in the group
* we **combine** the data back together again

::: {.panel-tabset}
## pandas
In the code below, we can see that the groupby function splits the data into different groups (and later combines them again). the agg function applies an aggregation, in this case the unique count of patients in the group. There are a few more actions in this code, renaming the index to organization, and the patient_id column to # patients. 

```{python}
df_grouped_pandas = df_timeline_pandas.groupby("organization_name").agg(
    {"patient_id":pd.Series.nunique}
).rename(
    columns = {'patient_id':"# patients"}
).rename_axis('organization')

# show first 5 records 
df_grouped_pandas.head()
```


## polars
```{python}
df_grouped_polars = (
    df_timeline_polars
    .group_by("organization_name")
    .agg(
        pl.n_unique("patient_id")
    )
)
print(df_grouped_polars.head())
```

Polars supports both lazy and non-lazy (eager) evaluation. Lazy evaluation means that execution is deferred to the last minute, which can have significant performance advantages.

```{python}
df_grouped_polars = (
    df_timeline_polars.lazy()
    .group_by("organization_name")
    .agg(
        pl.n_unique("patient_id")
    )
    .collect()
)
print(df_grouped_polars)
```

## SQL

```{python}
query = f"""
create or replace table organization_grouped as (
    select 
        organization_name as organization,
        count(distinct patient_id) as patient_count
    from patient_timeline
    group by organization_name
) ;
select * from organization_grouped 
limit 5;


"""
con_duckdb.sql(query).to_df()
```

## IBIS

```{python}
df_grouped_ibis = df_timeline_IBIS.group_by("organization_name").aggregate(df_timeline_IBIS.patient_id.nunique()).to_pandas()
df_grouped_ibis.head()

```

```{python}
df_grouped_ibis = df_timeline_IBIS.group_by("organization_name").aggregate(df_timeline_IBIS.patient_id.nunique())
ibis.show_sql(df_grouped_ibis)
```

## pyspark

```{python}
#option 1, using buid in functions
df_grouped_pyspark = df_timeline_spark.groupBy('organization_name').agg(
        F.countDistinct('patient_id'
    ).alias('# patients')).orderBy(
        '# patients',ascending = False
    )

df_grouped_pyspark = df_grouped_pyspark.withColumnRenamed("organization_name", "organization")

# option 2: using SQL 
df_timeline_spark.createOrReplaceTempView("timeline")
df_grouped_pyspark = spark.sql('''
Select 
    organization_name, 
    count(distinct patient_id) as nr_of_patients 
from timeline group by organization_name 
order by count(distinct patient_id) DESC
''')

df_grouped_pyspark.limit(5).show()

```


## dplyr
```{python}
%%R
df_grouped <- df %>% 
  group_by(organization_name) %>%
  summarise(
    number_of_patients = n_distinct(patient_id),
    .groups = "drop"
  )
kable(head(df_grouped))
```

:::

### Filter
In data analysis it also frequently occurs that you are interested in only a subset of the data. For that reason we apply the filter technique. From our patients we only want to keep the well child visits. 

::: {.panel-tabset}
## pandas

```{python}
# option 1
value = 'Well child visit (procedure)'
df_filtered_pandas = df_timeline_pandas[df_timeline_pandas['procedure_name'] == value]

# option 2
df_filtered_pandas = df_timeline_pandas.loc[df_timeline_pandas['procedure_name'] == value]

# option 3
df_filtered_pandas = df_filtered_pandas.query(f"procedure_name == '{value}' ")

#show results
df_filtered_pandas[['patient_id','start_time','procedure_name']].head()
```


## polars
```{python}
df_filtered_polars = (
    df_timeline_polars
    .filter(pl.col("procedure_name") == "Well child visit (procedure)")
)
print(df_filtered_polars.head())
```

## SQL

```{python}
query = """
create or replace table filtered_table as (
    Select 
        * 
    from patient_timeline 
    where procedure_name = 'Well child visit (procedure)'
);
select * from filtered_table limit 5;
"""
con_duckdb.sql(query).to_df()
```

## IBIS

```{python}
df_filtered_ibis = df_timeline_IBIS.filter(df_timeline_IBIS.procedure_name == "Well child visit (procedure)")
df_filtered_ibis.to_pandas().head()

```


## pyspark

```{python}
# option 1
value = 'Well child visit (procedure)'
df_filtered_pyspark = df_timeline_spark.filter(df_timeline_spark['procedure_name']==value)

# option 2
df_filtered_pyspark = df_timeline_spark.filter(F.expr(f"procedure_name == '{value}'"))

# option 3 - sql
df_timeline_spark.createOrReplaceTempView("timeline")
df_filtered_pyspark = spark.sql(f"Select * from timeline where procedure_name =='{value}'")

#show results
df_filtered_pyspark[['patient_id','start_time','procedure_name']].limit(5).show()
```

## dplyr
```{python}
%%R
df_filtered <- df %>%
  filter(procedure_name == "Well child visit (procedure)")
head(df_filtered)
```


:::

Besides filtering data by content it is also possible to filter the columns one wishes to show

::: {.panel-tabset}
## pandas

```{python}
df_filtered_pandas.filter(items = ['patient_id','birthDate','organization_name']).head(1)
```

```{python}
df_filtered_pandas[['patient_id','birthDate','organization_name']].head(1)
```


## polars
```{python}
df_filtered_polars.select([
    pl.col("patient_id"), pl.col("birthDate"), pl.col("organization_name")
]).head()
```

## SQL

```{python}
query = """
Select 
    patient_id,
    birthDate,
    organization_name
from patient_timeline limit 1
"""
con_duckdb.sql(query).to_df()
```

## IBIS

```{python}
df_filtered_ibis.select(
    'patient_id','birthDate','organization_name'
).to_pandas().head()
```


## pyspark

```{python}
df_filtered_pyspark.select('patient_id','birthDate','organization_name').limit(1).show()
```

## dplyr
```{python}
#TODO
```

:::

### Mutate
Another common action, is that you want create adapt data. For example, we want to create another column with the year that a visit occured in the patient timeline, based on the start_time that is available. 
We see that currently the date is in a specific timezone, you need to take this into account when converting it in a datatime format. For now we keep the date in the same timezone, before extracting the year from it.


::: {.panel-tabset}
## pandas

```{python}
# transform start_time to datetime (its currently string format)
df_timeline_pandas['start_time'] = pd.to_datetime(df_timeline_pandas['start_time'], utc=  True)
df_timeline_pandas['start_time'].dtype

#option 1
df_timeline_pandas['year'] = df_timeline_pandas['start_time'].dt.year

#option 2
df_timeline_pandas = df_timeline_pandas.assign(year=df_timeline_pandas['start_time'].dt.year )

df_timeline_pandas[['start_time','year']].head(1)
```


## polars
```{python}
df_new_var_polars = (
    df_timeline_polars
    .with_columns(
        pl.col("start_time").str.to_datetime(format="%Y-%m-%dT%H:%M:%S%z")
        .dt.year().alias("start_year")
    )
)
print(df_new_var_polars.head())
```

## SQL

```{python}
query = """
Select
    cast(start_time as datetime) as start_time,
    year(cast(start_time as datetime)) as year
from patient_timeline;
"""
con_duckdb.sql(query).to_df()
```

## IBIS

```{python}
df_new_var_ibis = df_timeline_IBIS.mutate(start_year=df_timeline_IBIS.start_time.cast("timestamp").year()).to_pandas()
df_new_var_ibis.head()

```


## pyspark

```{python}
# transform start_time to datetime (its currently string format)
df_timeline_spark = df_timeline_spark.withColumn("start_time", F.to_utc_timestamp(df_timeline_spark["start_time"], "Europe/Amsterdam"))

df_timeline_spark = df_timeline_spark.withColumn("year", F.year(df_timeline_spark["start_time"]))
df_timeline_spark.select('start_time','year').limit(3).show()

```


## dplyr
```{python}
%%R
df_new_var <- df %>%
  mutate(start_year = year(as.Date(start_time, format = "%Y-%m-%d")))
head(df_new_var)
```


:::

### Order
It often occurs that you want to order your data. In this case let's order the patient timeline by start_time. Most libraries automatically order in an ascending/asc order: oldest first, and alow you to order descending/desc (newest first). 

::: {.panel-tabset}
## pandas

```{python}
df_ordered_pandas = df_timeline_pandas.sort_values(by = 'start_time', ascending = False)
df_ordered_pandas[['patient_id','start_time','procedure_name']].head()
```


## polars
```{python}
df_ordered_polars = (
    df_timeline_polars
    .with_columns(
        pl.col("start_time").str.to_datetime(format="%Y-%m-%dT%H:%M:%S%z")
    )
    .sort("start_time")
)
print(df_ordered_polars.head())
```

## SQL

```{python}
query = """
Select 
    * 
from patient_timeline
order by start_time asc 
limit 5
"""
con_duckdb.sql(query).to_df()
```

## IBIS

```{python}
df_ordered_ibis = df_timeline_IBIS.order_by(df_timeline_IBIS.start_time.cast("timestamp")).to_pandas()
df_ordered_ibis.head()

```


## pyspark

```{python}
df_ordered_pyspark = df_timeline_spark.orderBy('start_time',ascending = False)
df_ordered_pyspark.limit(5).show()
```

## dplyr
```{python}
%%R
df_ordered <- df %>%
  mutate(start_time = as.Date(start_time), format = "%Y-%m-%d") %>%
  arrange(start_time)
head(df_ordered)
```


:::

### Rename
Another action that is done frequently is the renaming of a column. As an example we rename the birthDate column to birth_date

::: {.panel-tabset}
## pandas

In pandas,  many actions such as *rename* can be adapted directly in the same dataframe by adding the option inplace = True. 

Alternatively, one can create a new dataframe and keep the name the same in the original dataframe.

```{python}
#option 1 - change in dataframe
df_timeline_pandas.rename(columns = {'birthDate':'birth_date'}, inplace = True)

#option 2 - change only in new dataframe
df_renamed_pandas = df_timeline_pandas.rename(columns = {'birthDate':'birth_date'})
df_renamed_pandas.head(1)
```


## polars
```{python}
df_renamed_polars = (
    df_timeline_polars
    .rename({"birthDate": "birth_date"})
)
print(df_renamed_polars.head(1))
```

## SQL

```{python}
query = """
alter table patient_timeline
rename column birthDate to birth_date;

select * from patient_timeline limit 5;
"""
print(con_duckdb.sql(query).to_df())

con_duckdb.sql("alter table patient_timeline rename column birth_date to birthDate")
```


## IBIS

```{python}

df_renamed_ibis = df_timeline_IBIS.relabel({"birthDate": "birth_date"}).to_pandas()
df_renamed_ibis.head(1)
```


## pyspark

```{python}
df_renamed_pyspark = df_timeline_spark.withColumnRenamed("birthDate", "birth_date")
df_renamed_pyspark.head(1)
```

## dplyr
```{python}
%%R
df_renamed <- df %>%
  rename(birth_date = birthDate)
head(df_renamed)
```

:::


### Join
It’s rare that a data analysis involves only a single data frame. Typically you have many data frames, and you must join them together to answer the questions that you’re interested in.

An inner join keeps observations that appear in both tables. 

![inner join](images/join-inner.png){#inner-join}

An outer join keeps observations that appear in at least one of the tables. There are three types of outer joins:

* A left join keeps all observations in x.
* A right join keeps all observations in y.
* A full join keeps all observations in x and y.

![outer join](images/join-outer.png){#outer-join}

The most commonly used join is the left join: you use this whenever you look up additional data from another table, because it preserves the original observations even when there isn’t a match. The left join should be your default join: use it unless you have a strong reason to prefer one of the others.

::: {.panel-tabset}
## pandas

```{python}
df_joined_pandas = pd.merge(
    df_timeline_pandas,
    df_price_list_pandas, 
    left_on = ['vaccine_code','vaccine_code_system'], 
    right_on = ['code','system'], 
    how ='left'
)

df_joined_pandas.filter(items = ['patient_id','start_time','year','vaccine_name','item_claimed','USD']).head()
```


## polars
```{python}
df_joined_polars = (
    df_timeline_polars
    .join(
        df_price_list_polars,
        left_on=["vaccine_code", "vaccine_code_system"],
        right_on=["code", "system"],
        how="left"
    )
)
print(df_joined_polars.head())
```

## SQL

```{python}
query = """
Select 
* 
from patient_timeline pt
left join price_list pl on pt.vaccine_code = pl.code and pt.vaccine_code_system = pl.system
limit 5;
"""
con_duckdb.sql(query).to_df()
```


## IBIS

```{python}
df_joined_ibis = df_timeline_IBIS.left_join(
    df_price_list_ibis, [
        df_timeline_IBIS.vaccine_code == df_price_list_ibis.code,
        df_timeline_IBIS.vaccine_code_system == df_price_list_ibis.system
    ]
).to_pandas()
df_joined_ibis.head()

```


## pyspark

```{python}
df_joined_pyspark = df_timeline_spark.join(df_price_list_spark,(df_timeline_spark['vaccine_code'] == df_price_list_spark['code']) & (df_timeline_spark['vaccine_code_system'] == df_price_list_spark['system']),'inner')

df_joined_pyspark.limit(5).show()

```

## dplyr
```{python}
%%R
df_joined <- df %>%
  left_join(
    df_price_list,
    by = c("vaccine_code" = "code", "vaccine_code_system" = "system")
  )
head(df_joined)
```

:::


### Finishing up

When using duckdb it is very important to always close the database connection. If one is connected to the databas a wall is created to block anyone else from connecting at the same time. This prevents conflicts when more are manipulating the data at the same time. 

```{python}
#close connection to the duckdb database
con_duckdb.close()
```