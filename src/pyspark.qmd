# Datawrangling using Pyspark

## Pyspark
Intro pyspark

## Example using the Synthea dataset
### load required libraries
```{python}
import pyspark
import duckdb 
from pyspark.sql import SparkSession
from pyspark.sql.types import StructType,StructField, StringType, DateType,TimestampType,IntegerType
from pyspark.sql import functions as F
from pathlib import Path
```

### connect to database and retrieve data


```{python}
# Set paths 

ROOT = Path('.')
BRONZE = ROOT / 'data' / 'bronze'
SILVER = ROOT / 'data' / 'silver'


# create spark session
spark = SparkSession.builder.appName("patient_timeline_analysis").getOrCreate()

# convert timeline to spark dataframe
df= spark.read.parquet(f'{SILVER}/parquet_export/patient_timeline.parquet')

# convert pricelist to spark dataframe
df_price_list = spark.read.parquet(f'{SILVER}/parquet_export/price_list.parquet')
```

### Group by, summarise

Count the number of patients per organization

* group by organization
* aggregate: unique patient_id count
* rename the patient_id aggregate to # patients
* rename the index title
* order by # of patients

```{python}
#option 1, using buid in functions
df_grouped = df.groupBy('organization_name').agg(
        F.countDistinct('patient_id'
    ).alias('# patients')).orderBy(
        '# patients',ascending = False
    )

df_grouped = df_grouped.withColumnRenamed("organization_name", "organization")

# show first 5 records 
df_grouped.limit(5).show()
```

```{python}
# option 2: using SQL 
df.createOrReplaceTempView("timeline")
df_grouped = spark.sql('''
Select 
    organization_name, 
    count(distinct patient_id) as nr_of_patients 
from timeline group by organization_name 
order by count(distinct patient_id) DESC
''')
df_grouped.limit(5).show()
```

### Filter

Filter patients by procedure


```{python}
# option 1
value = 'Well child visit (procedure)'
filtered_df = df.filter(df['procedure_name']==value)
```


```{python}
# option 2
filtered_df = df.filter(F.expr(f"procedure_name == '{value}'"))
```


```{python}
# option 3 - sql
df.createOrReplaceTempView("timeline")
filtered_df = spark.sql(f"Select * from timeline where procedure_name =='{value}'")
```

```{python}
#show results
filtered_df[['patient_id','start_time','procedure_name']].limit(5).show()
```

Next to filtering the dataframe on content, it is also possible to select which columns or rows to show.  

Only show the columns patient_id, birthDate, organization_name

```{python}
filtered_df.select('patient_id','birthDate','organization_name').limit(1).show()
```

### Mutate

Extract year from the start time

```{python}
# transform start_time to datetime (its currently string format)
df = df.withColumn("start_time", F.to_utc_timestamp(df["start_time"], "Europe/Amsterdam"))
```

```{python}
df = df.withColumn("year", F.year(df["start_time"]))
df.select('start_time','year').limit(3).show()
```

### Order

Order by start time newest first

```{python}
df_ordered = df.orderBy('start_time',ascending = False)
df_ordered.limit(5).show()
```

### Rename

Rename birthDate column to birth_date

In pandas,  many actions such as *rename* can be adapted directly in the same dataframe by adding the option inplace = True. 

Alternatively, one can create a new dataframe and keep the name the same in the original dataframe.

```{python}
df_renamed = df.withColumnRenamed("birthDate", "birth_date")
```



### Join 

Join the patient timeline with the pricelist table on code and system

```{python}

df_joined = df.join(df_price_list,(df['vaccine_code'] == df_price_list['code']) & (df['vaccine_code_system'] == df_price_list['system']),'inner')

df_joined.limit(5).show()
```