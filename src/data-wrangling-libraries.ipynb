{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Datawrangling\n",
        "In data analysis, we often repeat similar actions to get valuable information from the data. In this Chapter we describe the most common techniques applied in data analysis, and how they are performed in different programming languages and libraries.\n",
        "\n",
        "## Split-apply-combine\n",
        "Many data analysis problems involve the application of a split-apply-combine strategy,\n",
        "where you break up a big problem into manageable pieces, operate on each piece independently and then put all the pieces back together. group_by is the  most common function name for split-apply-combine technique.\n",
        "\n",
        "These actions can be done with any library, in multiple programming languages. \n",
        "Based on the sample data created in **TODO**, we will go through some standard data analysis actions. \n",
        "We will show how to apply these actions in multiple libraries and languages. \n",
        "\n",
        "## Datawrangling languages and libraries\n",
        "In this chapter we use the following languages and libraries:\n",
        "\n",
        "**Pandas 2.0**  \n",
        "Pandas is a popular python data wrangling library in the data science comunity. Pandas 2.0 mainly introduces improvements in performance, making it faster and more memory-efficient. \n",
        "\n",
        "**Polars**  \n",
        "\n",
        "**IBIS**  \n",
        "\n",
        "**PySpark**\n",
        "\n",
        "**Dplyr**  \n",
        "Dplyr is a widely used library in r.\n",
        "\n",
        "**SQL**  \n",
        "Duckdb\n",
        "\n",
        "## Overview data transformations in different libraries\n",
        "\n",
        "## Example using the Synthea dataset\n",
        "To demonstrate a few data sciene techniques in the different languages and libraries, we use the data generated in **CHAPTER X**\n",
        "\n",
        "### load required libraries\n",
        "Before getting started, we need to load the required libraries. \n",
        "\n",
        "::: {.panel-tabset}\n",
        "\n",
        "## pandas\n"
      ],
      "id": "9ce2255d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "from pathlib import Path"
      ],
      "id": "901e3580",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## polars"
      ],
      "id": "67561f12"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# import polars as pl\n",
        "from pathlib import Path"
      ],
      "id": "a09c2c89",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## SQL\n",
        "\n",
        "\n",
        "## IBIS"
      ],
      "id": "a449368b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import ibis \n",
        "import pandas as pd\n",
        "import duckdb\n",
        "from pathlib import Path"
      ],
      "id": "d2691df5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## pyspark\n"
      ],
      "id": "6ca51613"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType,StructField, StringType, DateType,TimestampType,IntegerType\n",
        "from pyspark.sql import functions as F\n",
        "from pathlib import Path"
      ],
      "id": "ff07f859",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "### Retrieve data\n",
        "\n",
        "In this section we will retrieve the data from the parquet files created in **Chapter X**.\n",
        "\n",
        "First we set the paths where the data is located"
      ],
      "id": "18c60016"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# set paths\n",
        "\n",
        "\n",
        "# TODO: is there a better way?\n",
        "# path from which code is run\n",
        "# when running entire notebook, this is from the main folder, \n",
        "# when running file locally, this is from the source folder\n",
        "ROOT = Path('.')\n",
        "if 'src' in str(ROOT.resolve()): # locally\n",
        "    ROOT = '..' / ROOT # go one folder up\n",
        "BRONZE = ROOT / 'data' / 'bronze'\n",
        "SILVER = ROOT / 'data' / 'silver'"
      ],
      "id": "8504e81d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then we import the data in a dataframe\n",
        "\n",
        "::: {.panel-tabset}\n",
        "## pandas\n"
      ],
      "id": "08579836"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# load data from parquet\n",
        "df_timeline_pandas = pd.read_parquet(f\"{SILVER}/parquet_export/patient_timeline.parquet\")\n",
        "df_price_list_pandas = pd.read_parquet(f\"{SILVER}/parquet_export/price_list.parquet\")"
      ],
      "id": "b226843c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## polars"
      ],
      "id": "ffe78059"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# read parquet files in polars dateframe\n",
        "df_timeline_polars = pl.read_parquet(f\"{SILVER}/parquet_export/patient_timeline.parquet\")\n",
        "df_price_list_polars = pl.read_parquet(f\"{SILVER}/parquet_export/price_list.parquet\")"
      ],
      "id": "d16f9fee",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## sql\n",
        "\n",
        "\n",
        "## IBIS\n",
        "\n",
        "**TODO** Show that this is also possible directly on parquet files"
      ],
      "id": "cf5d5471"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# connect to duckdb using ibis\n",
        "con = ibis.duckdb.connect(f\"{SILVER}/pregnancy.duckdb\")\n",
        "#retrieve data\n",
        "df_timeline_IBIS = con.table(\"patient_timeline\")\n",
        "df_price_list_ibis = con.table(\"price_list\")"
      ],
      "id": "e8ded88f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## pyspark\n"
      ],
      "id": "cf3ad26e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# create spark session\n",
        "spark = SparkSession.builder.appName(\"patient_timeline_analysis\").getOrCreate()\n",
        "\n",
        "# convert timeline to spark dataframe\n",
        "df_timeline_spark = spark.read.parquet(f'{SILVER}/parquet_export/patient_timeline.parquet')\n",
        "\n",
        "# convert pricelist to spark dataframe\n",
        "df_price_list_spark = spark.read.parquet(f'{SILVER}/parquet_export/price_list.parquet')"
      ],
      "id": "721bd24f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "### Split-apply-combine\n",
        "From the patient timeline data, we want to know how many patients each organization has recorded. For this task we can use the split-apply-combine technique. \n",
        "\n",
        "* we **split** the data in groups, where each group is a organization\n",
        "* we **apply** a function to count the number of patients in the group\n",
        "* we **combine** the data back together again\n",
        "\n",
        "::: {.panel-tabset}\n",
        "## pandas\n",
        "In the code below, we can see that the groupby function splits the data into different groups (and later combines them again). the agg function applies an aggregation, in this case the unique count of patients in the group. There are a few more actions in this code, renaming the index to organization, and the patient_id column to # patients. \n"
      ],
      "id": "fcb9c70f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df_grouped_pandas = df_timeline_pandas.groupby(\"organization_name\").agg(\n",
        "    {\"patient_id\":pd.Series.nunique}\n",
        ").rename(\n",
        "    columns = {'patient_id':\"# patients\"}\n",
        ").rename_axis('organization')\n",
        "\n",
        "# show first 5 records \n",
        "df_grouped_pandas.head()"
      ],
      "id": "9de4ccd4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## polars"
      ],
      "id": "79bb12ce"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df_grouped_polars = (\n",
        "    df_timeline_polars\n",
        "    .group_by(\"organization_name\")\n",
        "    .agg(\n",
        "        pl.n_unique(\"patient_id\")\n",
        "    )\n",
        ")\n",
        "print(df_grouped_polars.head())"
      ],
      "id": "958a0b89",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Polars supports both lazy and non-lazy (eager) evaluation. Lazy evaluation means that execution is deferred to the last minute, which can have significant performance advantages.\n"
      ],
      "id": "0e131085"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df_grouped_polars = (\n",
        "    df_timeline_polars.lazy()\n",
        "    .group_by(\"organization_name\")\n",
        "    .agg(\n",
        "        pl.n_unique(\"patient_id\")\n",
        "    )\n",
        "    .collect()\n",
        ")\n",
        "print(df_grouped_polars)"
      ],
      "id": "c0e31981",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## sql\n",
        "\n",
        "\n",
        "## IBIS\n"
      ],
      "id": "5eb3a015"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df_grouped_ibis = df_timeline_IBIS.group_by(\"organization_name\").aggregate(df_timeline_IBIS.patient_id.nunique()).to_pandas()\n",
        "df_grouped_ibis.head()"
      ],
      "id": "5ca26379",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df_grouped_ibis = df_timeline_IBIS.group_by(\"organization_name\").aggregate(df_timeline_IBIS.patient_id.nunique())\n",
        "ibis.show_sql(df_grouped_ibis)"
      ],
      "id": "b9fbe520",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## pyspark\n"
      ],
      "id": "05fb0f5d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#option 1, using buid in functions\n",
        "df_grouped_pyspark = df_timeline_spark.groupBy('organization_name').agg(\n",
        "        F.countDistinct('patient_id'\n",
        "    ).alias('# patients')).orderBy(\n",
        "        '# patients',ascending = False\n",
        "    )\n",
        "\n",
        "df_grouped_pyspark = df_grouped_pyspark.withColumnRenamed(\"organization_name\", \"organization\")\n",
        "\n",
        "# option 2: using SQL \n",
        "df_timeline_spark.createOrReplaceTempView(\"timeline\")\n",
        "df_grouped_pyspark = spark.sql('''\n",
        "Select \n",
        "    organization_name, \n",
        "    count(distinct patient_id) as nr_of_patients \n",
        "from timeline group by organization_name \n",
        "order by count(distinct patient_id) DESC\n",
        "''')\n",
        "\n",
        "df_grouped_pyspark.limit(5).show()"
      ],
      "id": "48d0a61a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "### Filter\n",
        "In data analysis it also frequently occurs that you are interested in only a subset of the data. For that reason we apply the filter technique. From our patients we only want to keep the well child visits. \n",
        "\n",
        "::: {.panel-tabset}\n",
        "## pandas\n"
      ],
      "id": "c61a7cd9"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# option 1\n",
        "value = 'Well child visit (procedure)'\n",
        "df_filtered_pandas = df_timeline_pandas[df_timeline_pandas['procedure_name'] == value]\n",
        "\n",
        "# option 2\n",
        "df_filtered_pandas = df_timeline_pandas.loc[df_timeline_pandas['procedure_name'] == value]\n",
        "\n",
        "# option 3\n",
        "df_filtered_pandas = df_filtered_pandas.query(f\"procedure_name == '{value}' \")\n",
        "\n",
        "#show results\n",
        "df_filtered_pandas[['patient_id','start_time','procedure_name']].head()"
      ],
      "id": "5286d832",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## polars"
      ],
      "id": "b2b84903"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df_filtered_polars = (\n",
        "    df_timeline_polars\n",
        "    .filter(pl.col(\"procedure_name\") == \"Well child visit (procedure)\")\n",
        ")\n",
        "print(df_filtered_polars.head())"
      ],
      "id": "32be544e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## sql\n",
        "\n",
        "\n",
        "## IBIS\n"
      ],
      "id": "52b1ff95"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df_filtered_ibis = df_timeline_IBIS.filter(df_timeline_IBIS.procedure_name == \"Well child visit (procedure)\").to_pandas()\n",
        "df_filtered_ibis.head()"
      ],
      "id": "4e02e4cc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## pyspark\n"
      ],
      "id": "5b1051e8"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# option 1\n",
        "value = 'Well child visit (procedure)'\n",
        "df_filtered_pyspark = df_timeline_spark.filter(df_timeline_spark['procedure_name']==value)\n",
        "\n",
        "# option 2\n",
        "df_filtered_pyspark = df_timeline_spark.filter(F.expr(f\"procedure_name == '{value}'\"))\n",
        "\n",
        "# option 3 - sql\n",
        "df_timeline_spark.createOrReplaceTempView(\"timeline\")\n",
        "df_filtered_pyspark = spark.sql(f\"Select * from timeline where procedure_name =='{value}'\")\n",
        "\n",
        "#show results\n",
        "df_filtered_pyspark[['patient_id','start_time','procedure_name']].limit(5).show()"
      ],
      "id": "1e45e4cd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "Besides filtering data by content it is also possible to filter the columns one wishes to show\n",
        "\n",
        "::: {.panel-tabset}\n",
        "## pandas\n"
      ],
      "id": "70d0edd6"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df_filtered_pandas.filter(items = ['patient_id','birthDate','organization_name']).head(1)"
      ],
      "id": "3c69a987",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df_filtered_pandas[['patient_id','birthDate','organization_name']].head(1)"
      ],
      "id": "d21983d6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## polars"
      ],
      "id": "27439297"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## sql\n",
        "\n",
        "\n",
        "## IBIS\n"
      ],
      "id": "d37e3747"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## pyspark\n"
      ],
      "id": "e2108b90"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df_filtered_pyspark.select('patient_id','birthDate','organization_name').limit(1).show()"
      ],
      "id": "a4995aba",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "### Mutate\n",
        "Another common action, is that you want create adapt data. For example, we want to create another column with the year that a visit occured in the patient timeline, based on the start_time that is available. \n",
        "\n",
        "\n",
        "::: {.panel-tabset}\n",
        "## pandas\n"
      ],
      "id": "9cb75493"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# transform start_time to datetime (its currently string format)\n",
        "df_timeline_pandas['start_time'] = pd.to_datetime(df_timeline_pandas['start_time'], utc=  True)\n",
        "df_timeline_pandas['start_time'].dtype\n",
        "\n",
        "#option 1\n",
        "df_timeline_pandas['year'] = df_timeline_pandas['start_time'].dt.year\n",
        "\n",
        "#option 2\n",
        "df_timeline_pandas = df_timeline_pandas.assign(year=df_timeline_pandas['start_time'].dt.year )\n",
        "\n",
        "df_timeline_pandas[['start_time','year']].head(1)"
      ],
      "id": "0d52e464",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## polars"
      ],
      "id": "690b8893"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df_new_var_polars = (\n",
        "    df_timeline_polars\n",
        "    .with_columns(\n",
        "        pl.col(\"start_time\").str.to_datetime(format=\"%Y-%m-%dT%H:%M:%S%z\")\n",
        "        .dt.year().alias(\"start_year\")\n",
        "    )\n",
        ")\n",
        "print(df_new_var_polars.head())"
      ],
      "id": "81d21067",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## sql\n",
        "\n",
        "\n",
        "## IBIS\n"
      ],
      "id": "5a8a6b7a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df_new_var_ibis = df_timeline_IBIS.mutate(start_year=df_timeline_IBIS.start_time.cast(\"timestamp\").year()).to_pandas()\n",
        "df_new_var_ibis.head()"
      ],
      "id": "4d9466bc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## pyspark\n"
      ],
      "id": "59e5766f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# transform start_time to datetime (its currently string format)\n",
        "df_timeline_spark = df_timeline_spark.withColumn(\"start_time\", F.to_utc_timestamp(df_timeline_spark[\"start_time\"], \"Europe/Amsterdam\"))\n",
        "\n",
        "df_timeline_spark = df_timeline_spark.withColumn(\"year\", F.year(df_timeline_spark[\"start_time\"]))\n",
        "df_timeline_spark.select('start_time','year').limit(3).show()"
      ],
      "id": "e0d903c7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "### Order\n",
        "It often occurs that you want to order your data. In this case let's order the patient timeline by start_time.\n",
        "\n",
        "::: {.panel-tabset}\n",
        "## pandas\n"
      ],
      "id": "c51c9917"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df_ordered_pandas = df_timeline_pandas.sort_values(by = 'start_time', ascending = False)\n",
        "df_ordered_pandas[['patient_id','start_time','procedure_name']].head()"
      ],
      "id": "200a260b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## polars"
      ],
      "id": "d4fa933c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df_ordered_polars = (\n",
        "    df_timeline_polars\n",
        "    .with_columns(\n",
        "        pl.col(\"start_time\").str.to_datetime(format=\"%Y-%m-%dT%H:%M:%S%z\")\n",
        "    )\n",
        "    .sort(\"start_time\")\n",
        ")\n",
        "print(df_ordered_polars.head())"
      ],
      "id": "e73f99da",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## sql\n",
        "\n",
        "\n",
        "## IBIS\n"
      ],
      "id": "dde2c900"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df_ordered_ibis = df_timeline_IBIS.order_by(df_timeline_IBIS.start_time.cast(\"timestamp\")).to_pandas()\n",
        "df_ordered_ibis.head()"
      ],
      "id": "ec805241",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## pyspark\n"
      ],
      "id": "1edc188c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df_ordered_pyspark = df_timeline_spark.orderBy('start_time',ascending = False)\n",
        "df_ordered_pyspark.limit(5).show()"
      ],
      "id": "92d0b228",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "### Rename\n",
        "Another action that is done frequently is the renaming of a column. As an example we rename the birthDate column to birth_date\n",
        "\n",
        "::: {.panel-tabset}\n",
        "## pandas\n",
        "\n",
        "In pandas,  many actions such as *rename* can be adapted directly in the same dataframe by adding the option inplace = True. \n",
        "\n",
        "Alternatively, one can create a new dataframe and keep the name the same in the original dataframe.\n"
      ],
      "id": "86dcc79b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#option 1 - change in dataframe\n",
        "df_timeline_pandas.rename(columns = {'birthDate':'birth_date'}, inplace = True)\n",
        "\n",
        "#option 2 - change only in new dataframe\n",
        "df_renamed_pandas = df_timeline_pandas.rename(columns = {'birthDate':'birth_date'})\n",
        "df_renamed_pandas.head(1)"
      ],
      "id": "a29caf04",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## polars"
      ],
      "id": "d9739fd3"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df_renamed_polars = (\n",
        "    df_timeline_polars\n",
        "    .rename({\"birthDate\": \"birth_date\"})\n",
        ")\n",
        "print(df_renamed_polars.head(1))"
      ],
      "id": "01a3b6f0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## sql\n",
        "\n",
        "\n",
        "## IBIS\n"
      ],
      "id": "14ba26c9"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df_renamed_ibis = df_timeline_IBIS.relabel({\"birthDate\": \"birth_date\"}).to_pandas()\n",
        "df_renamed_ibis.head(1)"
      ],
      "id": "feb3907b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## pyspark\n"
      ],
      "id": "eea15df3"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df_renamed_pyspark = df_timeline_spark.withColumnRenamed(\"birthDate\", \"birth_date\")\n",
        "df_renamed_pyspark.head(1)"
      ],
      "id": "3ae99147",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "\n",
        "### Join\n",
        "It’s rare that a data analysis involves only a single data frame. Typically you have many data frames, and you must join them together to answer the questions that you’re interested in.\n",
        "\n",
        "**inner join**\n",
        "https://d33wubrfki0l68.cloudfront.net/3abea0b730526c3f053a3838953c35a0ccbe8980/7f29b/diagrams/join-inner.png\n",
        "\n",
        "**left join**\n",
        "An inner join keeps observations that appear in both tables. An outer join keeps observations that appear in at least one of the tables. There are three types of outer joins:\n",
        "\n",
        "* A left join keeps all observations in x.\n",
        "* A right join keeps all observations in y.\n",
        "* A full join keeps all observations in x and y.\n",
        "\n",
        "\n",
        "The most commonly used join is the left join: you use this whenever you look up additional data from another table, because it preserves the original observations even when there isn’t a match. The left join should be your default join: use it unless you have a strong reason to prefer one of the others.\n",
        "\n",
        "https://d33wubrfki0l68.cloudfront.net/9c12ca9e12ed26a7c5d2aa08e36d2ac4fb593f1e/79980/diagrams/join-outer.png\n",
        "\n",
        "**right join**\n",
        "\n",
        "**outer join**\n",
        "\n",
        "\n",
        "::: {.panel-tabset}\n",
        "## pandas\n"
      ],
      "id": "35146779"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df_joined_pandas = pd.merge(\n",
        "    df_timeline_pandas,\n",
        "    df_price_list_pandas, \n",
        "    left_on = ['vaccine_code','vaccine_code_system'], \n",
        "    right_on = ['code','system'], \n",
        "    how ='left'\n",
        ")\n",
        "\n",
        "df_joined_pandas.filter(items = ['patient_id','start_time','year','vaccine_name','item_claimed','USD']).head()"
      ],
      "id": "79d3c1a1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## polars"
      ],
      "id": "ba022740"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df_joined_polars = (\n",
        "    df_timeline_polars\n",
        "    .join(\n",
        "        df_price_list_polars,\n",
        "        left_on=[\"vaccine_code\", \"vaccine_code_system\"],\n",
        "        right_on=[\"code\", \"system\"],\n",
        "        how=\"left\"\n",
        "    )\n",
        ")\n",
        "print(df_joined_polars.head())"
      ],
      "id": "8da7a62b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## sql\n",
        "\n",
        "\n",
        "## IBIS\n"
      ],
      "id": "1cc92a9f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df_joined_ibis = df_timeline_IBIS.left_join(\n",
        "    df_price_list_ibis, [\n",
        "        df_timeline_IBIS.vaccine_code == df_price_list_ibis.code,\n",
        "        df_timeline_IBIS.vaccine_code_system == df_price_list_ibis.system\n",
        "    ]\n",
        ").to_pandas()\n",
        "df_joined_ibis.head()"
      ],
      "id": "97422415",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## pyspark\n"
      ],
      "id": "647698db"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df_joined_pyspark = df_timeline_spark.join(df_price_list_spark,(df_timeline_spark['vaccine_code'] == df_price_list_spark['code']) & (df_timeline_spark['vaccine_code_system'] == df_price_list_spark['system']),'inner')\n",
        "\n",
        "df_joined_pyspark.limit(5).show()"
      ],
      "id": "bc874ea4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::"
      ],
      "id": "8546559d"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}