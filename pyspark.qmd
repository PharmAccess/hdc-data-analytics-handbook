# Datawrangling using Pyspark

## Pyspark
Intro pyspark

## Example using the Synthea dataset
### load required libraries
```{python}
import pyspark
import duckdb 
from pyspark.sql import SparkSession
from pyspark.sql.types import StructType,StructField, StringType, DateType,TimestampType,IntegerType
from pyspark.sql import functions as F
```

### connect to database and retrieve data
```{python}
#connect to database
SILVER = "data/SILVER"
con = duckdb.connect(f"{SILVER}/pregnancy.duckdb")

# create a pyspark session
spark = SparkSession.builder.appName("patient_timeline_analysis").getOrCreate()

#get data
# first create a pandas dataframe 
df_pandas = con.sql('select * from patient_timeline').to_df()

#need to specify schema as it doesnt know how to deal with UUID type
schema = StructType([
    StructField("patient_id", StringType(), False),
    StructField('social_security_number',StringType(), True),
    StructField('prefix',StringType(), True),
    StructField('first_name',StringType(), True),
    StructField('last_name',StringType(), True),
    StructField('birthDate',DateType(), True),
    StructField('code',StringType(), True),
    StructField('system',StringType(), True),
    StructField('organization_id',StringType(), True),
    StructField('organization_name',StringType(), True),
    StructField('practitioner_name',StringType(), True),
    StructField('practitioner_id',StringType(), True),
    StructField('procedure_name',StringType(), True),
    StructField('start_time',StringType(), True),
    StructField('end_time',StringType(), True),
    StructField('Vaccine_name',StringType(), True),
    StructField('vaccine_code',StringType(), True),
    StructField('vaccine_code_system',StringType(), True)
])
df = spark.createDataFrame(df_pandas, schema = schema)

df_price_list = spark.createDataFrame(con.sql('select * from price_list').to_df())

# close the connection
con.close()
```

### Group by, summarise

Count the number of patients per organization

* group by organization
* aggregate: unique patient_id count
* rename the patient_id aggregate to # patients
* rename the index title
* order by # of patients

```{python}
#option 1, using buid in functions
df_grouped = df.groupBy('organization_name').agg(
        F.countDistinct('patient_id'
    ).alias('# patients')).orderBy(
        '# patients',ascending = False
    )

df_grouped = df_grouped.withColumnRenamed("organization_name", "organization")

# show first 5 records 
df_grouped.limit(5).show()
```

```{python}
# option 2: using SQL 
df.createOrReplaceTempView("timeline")
df_grouped = spark.sql('''
Select 
    organization_name, 
    count(distinct patient_id) as nr_of_patients 
from timeline group by organization_name 
order by count(distinct patient_id) DESC
''')
df_grouped.limit(5).show()
```

### Filter

Filter patients by procedure


```{python}
# option 1
value = 'Well child visit (procedure)'
filtered_df = df.filter(df['procedure_name']==value)
```


```{python}
# option 2
filtered_df = df.filter(F.expr(f"procedure_name == '{value}'"))
```


```{python}
# option 3 - sql
df.createOrReplaceTempView("timeline")
filtered_df = spark.sql(f"Select * from timeline where procedure_name =='{value}'")
```

```{python}
#show results
filtered_df[['patient_id','start_time','procedure_name']].limit(5).show()
```

Next to filtering the dataframe on content, it is also possible to select which columns or rows to show.  

Only show the columns patient_id, birthDate, organization_name

```{python}
filtered_df.select('patient_id','birthDate','organization_name').limit(1).show()
```

### Mutate

Extract year from the start time

```{python}
# transform start_time to datetime (its currently string format)
df = df.withColumn("start_time", F.to_utc_timestamp(df["start_time"], "Europe/Amsterdam"))
```

```{python}
df = df.withColumn("year", F.year(df["start_time"]))
df.select('start_time','year').limit(3).show()
```

### Order

Order by start time newest first

```{python}
df_ordered = df.orderBy('start_time',ascending = False)
df_ordered.limit(5).show()
```

### Rename

Rename birthDate column to birth_date

In pandas,  many actions such as *rename* can be adapted directly in the same dataframe by adding the option inplace = True. 

Alternatively, one can create a new dataframe and keep the name the same in the original dataframe.

```{python}
df_renamed = df.withColumnRenamed("birthDate", "birth_date")
```



### Join 

Join the patient timeline with the pricelist table on code and system

```{python}

df_joined = df.join(df_price_list,(df['vaccine_code'] == df_price_list['code']) & (df['vaccine_code_system'] == df_price_list['system']),'inner')

df_joined.limit(5).show()
```